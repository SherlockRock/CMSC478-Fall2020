{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Introduction to Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Dimensionality Reduction and PCA</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Dimensionality Reduction</b>\n",
    "    - Curse of Dimensionality\n",
    "    - Data Projection\n",
    "        - Reducing Reconstruction Error\n",
    "        - Maximizing Variance\n",
    "    - Principal Components Analysis (PCA)\n",
    "        - Eigenproblem (Eigen-vector and Eigen-value)\n",
    "        - Singular Value Decomposition (SVD)\n",
    "    - PCA for Compression\n",
    "    - Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality</center></h1>\n",
    "\n",
    "<center><img src=\"img/curse.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality</center></h1>\n",
    "\n",
    "<center><img src=\"img/curse-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>High-Dimensional Data</center></h1>\n",
    "\n",
    "### High Dimensions - Lots of features\n",
    "\n",
    "\n",
    "<center><img src=\"img/meg.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>High-Dimensional Data - fMRI</center></h1>\n",
    "\n",
    "<center><img src=\"img/fmri_scanner.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/fmri_scan.jpg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Images from Ref[6]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Dimensionality Reduction</center></h1>\n",
    "\n",
    "- In many practical applications of ML, it is desirable to reduce the **dimensionality** of the data:\n",
    "    - Data visualization\n",
    "    - Data exploration: for investigating the **effective** dimensionality of the data\n",
    "    - More efficient use of resources (e.g., time, memory, communication)\n",
    "    - Overfitting: fewer dimensions -> better generalization\n",
    "    - Noise removal (improving data quality)\n",
    "    - Preprocessing: further processing by ML models (e.g. image classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Dimensionality Reduction Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/dimensionality-reduction.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Principal Component Analysis (PCA) - Geometric Perspective</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-def.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-01.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA - Problem Definition</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-problem.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>First Principal Component</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-first.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Minimizing Reconstruction Error Equivalent to Maximizing the Variance</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-eq.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Reconstruction and Variance</center></h1>\n",
    "\n",
    "<center><img src=\"img/max-variance.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centering</center></h1>\n",
    "\n",
    "<center><img src=\"img/centering.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centered Data</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-02.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centered Data Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Projecting Data to Reduce Dimensionality</center></h1>\n",
    "\n",
    "- There are infinite number of choices (directions) for projecting the data; thus we need a criteria:\n",
    "\n",
    "    - Finding a direction with <font color=\"blue\"><b>Minimum Reconstruction Error</b></font> (minimum information loss)\n",
    "\n",
    "    - Equivalently, finding a direction with <font color=\"blue\"><b>Maximum Variance</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection - Maximizing the Variance</center></h1>\n",
    "\n",
    "### Consider the two projections below. Which one maximizes the variance?\n",
    "\n",
    "<center><img src=\"img/variance-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection - Reducing Reconstruction Error</center></h1>\n",
    "\n",
    "<center><img src=\"img/reconstruction.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA 1st Axis</center></h1>\n",
    "\n",
    "<center><img src=\"img/variance-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA 2nd Axis</center></h1>\n",
    "\n",
    "<center><img src=\"img/variance-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Eigenproblem</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Singular Value Decomposition (SVD)</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVD Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>SVD Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA for Compression</center></h1>\n",
    "\n",
    "<center><img src=\"img/compression.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernel PCA</center></h1>\n",
    "\n",
    "- With PCA, the implicit assumption is that data lies on a linear subspace. There are situations where this assumption might be violated.\n",
    "\n",
    "\n",
    "- Kernel trick (used for SVM) can be applied as a pre-step for PCA to obtain non-linear generalization of PCA.\n",
    "\n",
    "\n",
    "- KPCA makes it possible to perform complex nonlinear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Coming Up Next: Unsupervised Learning Techniques & Clustering</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019\n",
    "\n",
    "[2] PCA - [Slides from EPFL](http://lasa.epfl.ch/teaching/lectures/ML_Msc/Slides/PCA.pdf)\n",
    "\n",
    "[3] PCA - [Matt Gormley's Slides CMU](https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture14-pca.pdf) (some originally from Andrew Ng's Slides, Poczos' Slides and Nina Balcan's Slides)\n",
    "\n",
    "[4] PCA - [Benrstein's Slides NYU](https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/13-PCA-Slides.pdf)\n",
    "\n",
    "[5] PCA - [Rosasco's Slides MIT](http://lcsl.mit.edu/courses/mlcc/mlcc2015/slides/MLCC_05_PCA.pdf)\n",
    "\n",
    "[6] Introduction to fMRI - [University of Oxford](https://www.ndcn.ox.ac.uk/divisions/fmrib/what-is-fmri/introduction-to-fmri)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
