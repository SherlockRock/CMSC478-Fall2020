{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Introduction to Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Ensemble Methods and Random Forests</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Ensemble Methods:</b>\n",
    "    - Bootstrap Aggregating (Bagging) & Pasting\n",
    "        - Random Forests\n",
    "    - Boosting\n",
    "        - AdaBoost\n",
    "        - Gradient Boosting\n",
    "        - XGBoost\n",
    "    - Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Ensemble of Predictors - Classifiers or Regressors</center></h1>\n",
    "<h3><center>The Wisdom of the Crowd</center></h3>\n",
    "\n",
    "<center><img src=\"img/ensemble.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Hard Voting</center></h1>\n",
    "\n",
    "<center><img src=\"img/hard-voting.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The Law of Large Numbers</center></h1>\n",
    "\n",
    "<center><img src=\"img/coin-tosses.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Ensemble of Weak Learners</center></h1>\n",
    "\n",
    "- Ensemble methods work best when the predictors are **independent** from one another making **uncorrelated** errors.\n",
    "\n",
    "\n",
    "- One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n",
    "\n",
    "\n",
    "- <font color=\"blue\"><b>Soft Voting:</b></font> If all classifiers can estimate class probabilities (i.e., they all have a `predict_proba()` method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called **soft voting**.\n",
    "\n",
    "\n",
    "- <font color=\"blue\"><b>Soft Voting</b></font> often achieves higher performance than hard voting because it gives more weight to highly confident votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Bootstrap Aggregating (Bagging) and Pasting</center></h1>\n",
    "\n",
    "<center><img src=\"img/bagging.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Bagging & Pasting Cont.</center></h1>\n",
    "\n",
    "- The aggregation function is typically the **statistical mode** (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression.\n",
    "\n",
    "\n",
    "- Each individual predictor has a higher **bias** than if it were trained on the original training set, but aggregation reduces both **bias** and **variance**.\n",
    "\n",
    "\n",
    "- Generally, the net result is that the ensemble has a similar **bias** but a lower **variance** than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Bagging & Pasting  Cont.</center></h1>\n",
    "\n",
    "- Individual predictors can all be trained in parallel, via different CPU cores or even different servers.\n",
    "\n",
    "\n",
    "- Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: <font color=\"blue\">ensemble methods scale very well!</font>\n",
    "\n",
    "\n",
    "- Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher **bias** than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s **variance** is reduced.\n",
    "\n",
    "\n",
    "- Overall, **bagging** often results in better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Out Of Bag Evaluation - oob</center></h1>\n",
    "\n",
    "- With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all.\n",
    "\n",
    "\n",
    "- By default a `BaggingClassifier` samples **m** training instances with replacement ( `bootstrap=True` ), where **m** is the size of the training set.\n",
    "\n",
    "\n",
    "- This means that only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called <font color=\"blue\">out-of-bag (oob)</font> instances. Note that they are not the same 37% for all predictors.\n",
    "\n",
    "\n",
    "- As **m** grows, this ratio approaches `1 – exp(–1) ≈ 63.212%`. $1 - e^{-1} ≈ 63\\% $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>oob As Validation Set</center></h1>\n",
    "\n",
    "- Since a predictor never sees the **oob** instances during training, it can be evaluated on these instances, without the need for a separate validation set.\n",
    "\n",
    "\n",
    "- You can evaluate the ensemble itself by averaging out the **oob** evaluations of each predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Random Patches and Random Subspaces</center></h1>\n",
    "\n",
    "- Sampling both training instances and features is called the <font color=\"blue\">Random Patches</font> method.\n",
    "\n",
    "\n",
    "- Keeping all training instances (by setting `bootstrap=False` and `max_samples=1.0` ) but sampling features (by setting `bootstrap_features` to True and/or `max_features` to a value smaller than 1.0 ) is called the <font color=\"blue\">Random Subspaces</font> method.\n",
    "\n",
    "\n",
    "- Sampling features results in even more predictor diversity, trading a bit more **bias** for a lower **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Random Forests</center></h1>\n",
    "\n",
    "- <font color=\"blue\">Random Forest</font> is an **ensemble** of **Decision Trees**, generally trained via the **bagging** method.\n",
    "\n",
    "\n",
    "- The <font color=\"blue\">Random Forest</font> algorithm introduces extra **randomness** when growing trees:\n",
    "\n",
    "    - Instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "\n",
    "- The algorithm results in greater tree diversity, which trades a higher **bias** for a lower **variance**, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Extremely Randomized Trees</center></h1>\n",
    "\n",
    "- When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting.\n",
    "\n",
    "\n",
    "- It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do).\n",
    "\n",
    "\n",
    "- A forest of such extremely random trees is called an <font color=\"blue\">Extremely Randomized Trees </font> ensemble (or **Extra-Trees** for short).\n",
    "\n",
    "\n",
    "- Once again, this technique trades more **bias** for a lower **variance**.\n",
    "\n",
    "\n",
    "- It also makes **Extra-Trees** much faster to train than regular Random Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Feature Importance</center></h1>\n",
    "\n",
    "- Another great quality of **Random Forests** is that they make it easy to measure the relative importance of each feature.\n",
    "\n",
    "\n",
    "- Scikit-Learn measures a **feature’s importance** by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest).\n",
    "\n",
    "\n",
    "- More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Feature Importance for MNIST</center></h1>\n",
    "\n",
    "<center><img src=\"img/feature-importance.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Boosting</center></h1>\n",
    "\n",
    "- <font color=\"blue\">Boosting</font> (originally called hypothesis boosting) refers to any **Ensemble** method that can combine several **weak learners** into a strong learner.\n",
    "\n",
    "\n",
    "- The general idea of most <font color=\"blue\">Boosting</font> methods is to train predictors **sequentially**, each trying to correct its predecessor.\n",
    "\n",
    "\n",
    "- There are many <font color=\"blue\">Boosting</font> methods available, but by far the most popular are <font color=\"blue\">AdaBoost</font> (short for Adaptive Boosting) and <font color=\"blue\">Gradient Boosting</font> (and its variations like <font color=\"blue\">XGBoost</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost</center></h1>\n",
    "\n",
    "<center><img src=\"img/adaboost.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost Predictions</center></h1>\n",
    "\n",
    "- Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.\n",
    "\n",
    "\n",
    "- There is one important drawback to this sequential learning technique: it cannot be parallelized (or only partially), since each predictor can only be trained after the previous predictor has been trained and evaluated.\n",
    "\n",
    "\n",
    "- As a result, **AdaBoost** does not scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost Math</center></h1>\n",
    "\n",
    "- Each instance weight $w^{(i)}$ is initially set to $1/m$ m is the total number of instances\n",
    "\n",
    "\n",
    "**Equation 7-1: Weighted error rate $r_j$ of the $j^\\text{th}$ predictor**\n",
    "\n",
    "$\n",
    "r_j = \\dfrac{\\displaystyle \\sum\\limits_{\\textstyle {i=1 \\atop \\hat{y}_j^{(i)} \\ne y^{(i)}}}^{m}{w^{(i)}}}{\\displaystyle \\sum\\limits_{i=1}^{m}{w^{(i)}}} \\quad\n",
    "\\text{where }\\hat{y}_j^{(i)}\\text{ is the }j^{\\text{th}}\\text{ predictor's prediction for the }i^{\\text{th}}\\text{ instance.}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost Math Cont.</center></h1>\n",
    "\n",
    "- The more accurate the predictor is, the higher its weight will be.\n",
    "\n",
    "- If it is just guessing randomly, then its weight will be close to zero.\n",
    "\n",
    "- However, if it is most often wrong (i.e., less accurate than random guessing), then its weight will be negative.\n",
    "\n",
    "**Equation 7-2: Predictor weight**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "\\alpha_j = \\eta \\log{\\dfrac{1 - r_j}{r_j}}\n",
    "\\end{split}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost Math Cont.</center></h1>\n",
    "\n",
    "- Next, the AdaBoost algorithm updates the instance weights, using Equation 7-3, which boosts the weights of the misclassified instances.\n",
    "\n",
    "**Equation 7-3: Weight update rule**\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\text{ for } i = 1, 2, \\dots, m \\\\\n",
    "& w^{(i)} \\leftarrow\n",
    "\\begin{cases}\n",
    "w^{(i)} & \\text{if }\\hat{y_j}^{(i)} = y^{(i)}\\\\\n",
    "w^{(i)} \\exp(\\alpha_j) & \\text{if }\\hat{y_j}^{(i)} \\ne y^{(i)}\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "- Then all the instance weights are normalized (i.e., divided by $ \\sum_{i=1}^{m}{w^{(i)}} $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>AdaBoost Prediction Math</center></h1>\n",
    "\n",
    "- Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "\n",
    "- To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Equation 7-4: AdaBoost predictions**\n",
    "\n",
    "$\n",
    "\\hat{y}(\\mathbf{x}) = \\underset{k}{\\operatorname{argmax}}{\\sum\\limits_{\\scriptstyle j=1 \\atop \\scriptstyle \\hat{y}_j(\\mathbf{x}) = k}^{N}{\\alpha_j}} \\quad \\text{where }N\\text{ is the number of predictors.}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Gradient Boosting</center></h1>\n",
    "\n",
    "- Just like **AdaBoost**, <font color=\"blue\">Gradient Boosting</font> works by sequentially adding predictors to an ensemble, each one correcting its predecessor.\n",
    "\n",
    "\n",
    "- However, instead of tweaking the instance weights at every iteration like AdaBoost does, <font color=\"blue\">Gradient Boosting</font> tries to fit the new predictor to the **residual errors** made by the previous predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>XGBoost - Extreme Gradient Boosting</center></h1>\n",
    "\n",
    "- An optimized implementation of **Gradient Boosting** is available in the popular Python library <font color=\"blue\">[XGBoost](https://xgboost.readthedocs.io/en/latest/)</font>, which stands for **Extreme Gradient Boosting**.\n",
    "\n",
    "\n",
    "- This package was initially developed by [Tianqi Chen](https://arxiv.org/pdf/1603.02754.pdf) as part of the Distributed (Deep) Machine Learning Community (DMLC), and it aims to be extremely fast, scalable, and portable.\n",
    "\n",
    "\n",
    "- In fact, <font color=\"blue\">[XGBoost](http://dmlc.cs.washington.edu/xgboost.html)</font> is often an important component of the winning entries in ML competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stacking</center></h1>\n",
    "\n",
    "- <font color=\"blue\">Stacking</font> (short for stacked generalization) is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don’t we train a model to learn this aggregation?\n",
    "\n",
    "<center><img src=\"img/stacking-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stacking Cont.</center></h1>\n",
    "\n",
    "- To train the blender, a common approach is to use a <font color=\"blue\"><b>hold-out set</b></font> (Subset 2).\n",
    "\n",
    "<center><img src=\"img/stacking-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stacking Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/stacking-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Stacking Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/stacking-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Next Chapter - Dimensionality Reduction & PCA</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
